{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Using NLTK"
      ],
      "metadata": {
        "id": "BE0F8F4p-1zk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GARo4Reu-t_S",
        "outputId": "23896a2b-f453-40c9-8903-a236bc2980d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Hello', 'world', '!', 'NLP', 'is', 'amazing', '.', 'Let', \"'s\", 'tokenize', 'this', 'sentence', '.']\n",
            "Sentence Tokens: ['Hello world!', 'NLP is amazing.', \"Let's tokenize this sentence.\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Hello world! NLP is amazing. Let's tokenize this sentence.\"\n",
        "\n",
        "# Word Tokenization\n",
        "print(\"Word Tokens:\", word_tokenize(text))\n",
        "\n",
        "# Sentence Tokenization\n",
        "print(\"Sentence Tokens:\", sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Spacy"
      ],
      "metadata": {
        "id": "lKdMu41U-4bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "print(\"Word Tokens:\", [token.text for token in doc])\n",
        "print(\"Sentence Tokens:\", [sent.text for sent in doc.sents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7wvjpGn-5s6",
        "outputId": "9b0028de-5152-4308-beea-790271c0d92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Hello', 'world', '!', 'NLP', 'is', 'amazing', '.', 'Let', \"'s\", 'tokenize', 'this', 'sentence', '.']\n",
            "Sentence Tokens: ['Hello world!', 'NLP is amazing.', \"Let's tokenize this sentence.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP Tasks"
      ],
      "metadata": {
        "id": "VRCa38vI_BnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Word Count & Frequency Distribution"
      ],
      "metadata": {
        "id": "5xeOfOC0_DWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "words = word_tokenize(text.lower())\n",
        "word_freq = Counter(words)\n",
        "print(\"Total Word Count:\", len(words))\n",
        "print(\"Word Frequency Distribution:\", word_freq.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4x1LpMS-9pq",
        "outputId": "b811419d-c36d-438d-8d6e-322a20bede5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Word Count: 13\n",
            "Word Frequency Distribution: [('.', 2), ('hello', 1), ('world', 1), ('!', 1), ('nlp', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords removal with frequency analysis"
      ],
      "metadata": {
        "id": "cNprH4BS_KHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "filtered_freq = Counter(filtered_words)\n",
        "\n",
        "print(\"Filtered Word Frequency Distribution:\", filtered_freq.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyVXQ52w_Gfi",
        "outputId": "94b0107a-2d33-4de2-e606-8651d94c89a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Word Frequency Distribution: [('.', 2), ('hello', 1), ('world', 1), ('!', 1), ('nlp', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER with Context Analysis"
      ],
      "metadata": {
        "id": "7dDiXG2q_ZQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load spaCy model\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "doc = nlp(text)  # Process the text\n",
        "\n",
        "\n",
        "entity_freq = Counter([ent.text for ent in doc.ents])\n",
        "for ent, freq in entity_freq.most_common():\n",
        "    print(f\"Entity: {ent}, Count: {freq}, Label: {nlp(ent)[0].ent_type_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSFBXG0Q_Pmo",
        "outputId": "2840ecce-6adf-4bc6-d6cd-c31c9ec88137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: Apple, Count: 1, Label: ORG\n",
            "Entity: U.K., Count: 1, Label: GPE\n",
            "Entity: $1 billion, Count: 1, Label: MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-Grams with frequency analysis"
      ],
      "metadata": {
        "id": "K6vl3XvnBBZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "bigrams = list(ngrams(words, 2))\n",
        "trigrams = list(ngrams(words, 3))\n",
        "bigram_freq = Counter(bigrams)\n",
        "trigram_freq = Counter(trigrams)\n",
        "print(\"Most Common Bigrams:\", bigram_freq.most_common(5))\n",
        "print(\"Most Common Trigrams:\", trigram_freq.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2IRRWEx_bDi",
        "outputId": "2133e918-5dee-433b-9c02-cedb2b61f914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common Bigrams: [(('hello', 'world'), 1), (('world', '!'), 1), (('!', 'nlp'), 1), (('nlp', 'is'), 1), (('is', 'amazing'), 1)]\n",
            "Most Common Trigrams: [(('hello', 'world', '!'), 1), (('world', '!', 'nlp'), 1), (('!', 'nlp', 'is'), 1), (('nlp', 'is', 'amazing'), 1), (('is', 'amazing', '.'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keyword Extraction using TF-IDF with Tokenization"
      ],
      "metadata": {
        "id": "4eDKZJTQBJJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform([text])\n",
        "feature_array = vectorizer.get_feature_names_out()\n",
        "importance = X.toarray().flatten()\n",
        "important_words = sorted(zip(feature_array, importance), key=lambda x: x[1], reverse=True)[:5]\n",
        "print(\"Top Keywords:\", [word for word, _ in important_words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXMLomGgBEk2",
        "outputId": "032855b0-b878-4b23-94cc-a04bc971f233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Keywords: ['apple', 'at', 'billion', 'buying', 'for']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tagging"
      ],
      "metadata": {
        "id": "R6aZucChBOF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, POS: {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYCLwqj-BLZ8",
        "outputId": "48e8a1d3-bdba-421e-e88c-13473f3b34a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Apple, POS: PROPN\n",
            "Token: is, POS: AUX\n",
            "Token: looking, POS: VERB\n",
            "Token: at, POS: ADP\n",
            "Token: buying, POS: VERB\n",
            "Token: U.K., POS: PROPN\n",
            "Token: startup, POS: VERB\n",
            "Token: for, POS: ADP\n",
            "Token: $, POS: SYM\n",
            "Token: 1, POS: NUM\n",
            "Token: billion, POS: NUM\n",
            "Token: ., POS: PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Similarity"
      ],
      "metadata": {
        "id": "7WLKUG3IBTc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "sentences = [\"I love NLP\", \"NLP is great for text processing\", \"Machine learning is amazing\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "print(\"Sentence Similarity Matrix:\")\n",
        "print((X * X.T).toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sia66PmEBPWx",
        "outputId": "4184b7cc-f324-4a7f-84d9-6f7b9f9cbb7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Similarity Matrix:\n",
            "[[2 1 0]\n",
            " [1 6 1]\n",
            " [0 1 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Summarization"
      ],
      "metadata": {
        "id": "lIriUz0jBbHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores = {}\n",
        "for sent in doc.sents:\n",
        "    for word in word_tokenize(sent.text.lower()):\n",
        "        if word in filtered_freq:\n",
        "            sentence_scores[sent.text] = sentence_scores.get(sent.text, 0) + filtered_freq[word]\n",
        "sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "summary = \" \".join(sorted_sentences[:2])\n",
        "print(\"Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nooMLvDSBVfE",
        "outputId": "220e5a6f-e5d8-4ef4-e5b2-f33d60f35e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Apple is looking at buying U.K. startup for $1 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download required NLTK tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"Natural Language Processing (NLP) is a field of AI that enables machines to understand human language.\n",
        "It is used in chatbots, sentiment analysis, and language translation.\n",
        "One of the key challenges in NLP is understanding context and ambiguity in sentences.\n",
        "Deep learning models like transformers have significantly improved NLP applications.\"\"\"\n",
        "\n",
        "# Process text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize words and compute frequency\n",
        "words = [word.lower() for word in word_tokenize(text) if word.isalnum()]\n",
        "filtered_freq = Counter(words)\n",
        "\n",
        "# Compute sentence scores\n",
        "sentence_scores = {}\n",
        "for sent in doc.sents:\n",
        "    for word in word_tokenize(sent.text.lower()):\n",
        "        if word in filtered_freq:\n",
        "            sentence_scores[sent.text] = sentence_scores.get(sent.text, 0) + filtered_freq[word]\n",
        "\n",
        "# Sort sentences by score\n",
        "sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "\n",
        "# Select top-ranked sentences for summary\n",
        "summary = \" \".join(sorted_sentences[:2])\n",
        "\n",
        "# Print summary\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "id": "1jx2-D8WBcz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}