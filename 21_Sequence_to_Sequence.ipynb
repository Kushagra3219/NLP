{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBIcK3fGF4pL",
        "outputId": "d16aee7b-180d-4445-acc5-c12578259faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sequence (src):\n",
            "tf.Tensor(\n",
            "[[4282 6883 8253 3680 5773]\n",
            " [ 622 5774 9259 4044 8007]], shape=(2, 5), dtype=int32)\n",
            "Target Sequence (tgt):\n",
            "tf.Tensor(\n",
            "[[1229 2292 1321 6478 5968]\n",
            " [8997 7252 1391 1152 6234]], shape=(2, 5), dtype=int32)\n",
            "\n",
            "Output Shape:\n",
            "(2, 5, 10000)\n",
            "\n",
            "Output (before softmax):\n",
            "tf.Tensor(\n",
            "[[[ 0.00023152  0.00036751  0.00580649 ...  0.00133648 -0.00014612\n",
            "   -0.00182201]\n",
            "  [-0.00138948 -0.00268177 -0.0008823  ...  0.00316981  0.00413238\n",
            "   -0.00082111]\n",
            "  [-0.00474558 -0.00356304  0.00091932 ...  0.00477495  0.00299576\n",
            "   -0.00019762]\n",
            "  [ 0.00275621 -0.0026377   0.00417847 ... -0.00139687 -0.00213797\n",
            "    0.00158448]\n",
            "  [ 0.00095046 -0.00352572 -0.00298803 ...  0.00140957  0.00222611\n",
            "    0.00345636]]\n",
            "\n",
            " [[-0.00293655  0.00541074  0.0012671  ...  0.00159937  0.00400032\n",
            "   -0.00195865]\n",
            "  [-0.00218288  0.00186938 -0.00458215 ...  0.00118869  0.00689738\n",
            "   -0.00039638]\n",
            "  [-0.00051523 -0.00262186 -0.0012761  ...  0.00489513  0.00059553\n",
            "   -0.00020457]\n",
            "  [-0.00354018 -0.00196027 -0.00137286 ...  0.00438106  0.00178036\n",
            "   -0.00017965]\n",
            "  [ 0.00335472  0.00216232 -0.00202784 ...  0.00092325  0.00140034\n",
            "    0.00070343]]], shape=(2, 5, 10000), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "class Encoder(Model):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed = layers.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = layers.GRU(hidden_dim, return_state=True, return_sequences=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embed(x)\n",
        "        output, hidden = self.rnn(x)\n",
        "        return hidden\n",
        "\n",
        "class Decoder(Model):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = layers.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = layers.GRU(hidden_dim, return_state=True, return_sequences=True)\n",
        "        self.fc = layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embed(x)\n",
        "        output, hidden = self.rnn(x, initial_state=hidden)\n",
        "        return self.fc(output), hidden\n",
        "\n",
        "class Seq2Seq(Model):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def call(self, src, tgt):\n",
        "        hidden = self.encoder(src)\n",
        "        output, _ = self.decoder(tgt, hidden)\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "vocab_size = 10000  # Example vocab size\n",
        "emb_dim = 256       # Example embedding dimension\n",
        "hidden_dim = 512    # Example hidden dimension\n",
        "\n",
        "encoder = Encoder(vocab_size, emb_dim, hidden_dim)\n",
        "decoder = Decoder(vocab_size, emb_dim, hidden_dim)\n",
        "seq2seq = Seq2Seq(encoder, decoder)\n",
        "\n",
        "# Random input sequences: batch_size=2, sequence_length=5 (for simplicity)\n",
        "src = tf.random.uniform((2, 5), minval=0, maxval=vocab_size, dtype=tf.int32)  # Source sequence (batch_size=2, seq_len=5)\n",
        "tgt = tf.random.uniform((2, 5), minval=0, maxval=vocab_size, dtype=tf.int32)  # Target sequence (batch_size=2, seq_len=5)\n",
        "\n",
        "# Show input sequences\n",
        "print(\"Source Sequence (src):\")\n",
        "print(src)\n",
        "print(\"Target Sequence (tgt):\")\n",
        "print(tgt)\n",
        "\n",
        "# Forward pass through Seq2Seq model\n",
        "output = seq2seq(src, tgt)\n",
        "\n",
        "# Show output shape and some sample values\n",
        "print(\"\\nOutput Shape:\")\n",
        "print(output.shape)  # Expected: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "# Show the output values for inspection (for a more realistic scenario, you would pass through softmax layer for probabilities)\n",
        "print(\"\\nOutput (before softmax):\")\n",
        "print(output)"
      ]
    }
  ]
}