{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Evaluate a Sentiment Classifier (Naive Bayes + Feature Engineering)"
      ],
      "metadata": {
        "id": "kkp1DJSe4c1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNucKKb_pgKl",
        "outputId": "9f6f4307-1928-49f9-913a-98ae1af55132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.666\n",
            "Most Informative Features\n",
            "               insulting = True              neg : pos    =     13.3 : 1.0\n",
            "             outstanding = True              pos : neg    =     13.0 : 1.0\n",
            "            effortlessly = True              pos : neg    =     11.4 : 1.0\n",
            "                 freddie = True              neg : pos    =     11.2 : 1.0\n",
            "               ludicrous = True              neg : pos    =     11.2 : 1.0\n",
            "                 idiotic = True              neg : pos    =     10.8 : 1.0\n",
            "                  smooth = True              pos : neg    =     10.8 : 1.0\n",
            "                  verbal = True              pos : neg    =     10.8 : 1.0\n",
            "                  prinze = True              neg : pos    =     10.6 : 1.0\n",
            "              unbearable = True              neg : pos    =     10.6 : 1.0\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "import random\n",
        "\n",
        "nltk.download('movie_reviews')\n",
        "docs = [(list(movie_reviews.words(fileid)), category)\n",
        "        for category in movie_reviews.categories()\n",
        "        for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "random.shuffle(docs)\n",
        "\n",
        "def extract_features(words):\n",
        "    return {word: True for word in words}\n",
        "\n",
        "featuresets = [(extract_features(doc), category) for (doc, category) in docs]\n",
        "train_set, test_set = featuresets[:1500], featuresets[1500:]\n",
        "\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_set))\n",
        "classifier.show_most_informative_features(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram Phrase Extraction with Frequency Filtering"
      ],
      "metadata": {
        "id": "Lzrj0siI4uM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "\n",
        "words = movie_reviews.words()\n",
        "finder = BigramCollocationFinder.from_words(words)\n",
        "finder.apply_freq_filter(20)\n",
        "\n",
        "print(finder.nbest(BigramAssocMeasures.pmi, 10))  # Top 10 meaningful bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doe4I-Sz4eLM",
        "outputId": "69dfa923-c011-483d-ae3b-e98c27c445ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('del', 'toro'), ('salma', 'hayek'), ('san', 'francisco'), ('mortal', 'kombat'), ('charlize', 'theron'), ('ace', 'ventura'), ('natalie', 'portman'), ('ewan', 'mcgregor'), ('los', 'angeles'), ('darth', 'vader')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec-based Similarity Search using NLTK Corpus"
      ],
      "metadata": {
        "id": "EIsJ65-A40Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqUzAqUR42QF",
        "outputId": "e2ccc9bd-2c80-42e6-f422-7eb1fb9fe0bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh4EGAeG5LjW",
        "outputId": "e82702fb-b851-4b3e-9aa5-6cc7187133ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from nltk.corpus import brown\n",
        "\n",
        "sentences = brown.sents()\n",
        "model = gensim.models.Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=1)\n",
        "\n",
        "print(\"Similar to 'money':\", model.wv.most_similar(\"money\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v156Mxka4wVK",
        "outputId": "14ec653b-9977-45eb-ab39-f3f08646f943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar to 'money': [('care', 0.8275145888328552), ('job', 0.827122688293457), ('friendship', 0.8178503513336182), ('risk', 0.8029019236564636), ('joy', 0.7983232140541077), ('permission', 0.7982925772666931), ('anywhere', 0.7979304790496826), ('part-time', 0.7957543134689331), ('chances', 0.7951022386550903), ('bringing', 0.7930358648300171)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec for Document Classification"
      ],
      "metadata": {
        "id": "yoncNkxY46Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "docs = [TaggedDocument(words=movie_reviews.words(fileid), tags=[fileid])\n",
        "        for fileid in movie_reviews.fileids()]\n",
        "model = Doc2Vec(docs, vector_size=50, epochs=30)\n",
        "\n",
        "X = [model.dv[doc.tags[0]] for doc in docs]\n",
        "y = [movie_reviews.categories(fileid)[0] for fileid in movie_reviews.fileids()]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Doc2Vec Classifier Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD6mx4an46kl",
        "outputId": "64ea3232-3042-4626-9be6-7f99410cd287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc2Vec Classifier Accuracy: 0.665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PPMI Co-occurrence Vector Construction"
      ],
      "metadata": {
        "id": "A615nnA75Qx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "from nltk.util import bigrams\n",
        "\n",
        "text = \"dog cat bark meow dog bark meow cat\".split()\n",
        "vocab = list(set(text))\n",
        "co_matrix = Counter(bigrams(text))\n",
        "\n",
        "word_counts = Counter(text)\n",
        "ppmi_matrix = {}\n",
        "\n",
        "for (w1, w2), count in co_matrix.items():\n",
        "    p_w1 = word_counts[w1] / len(text)\n",
        "    p_w2 = word_counts[w2] / len(text)\n",
        "    p_w1_w2 = count / len(text)\n",
        "    pmi = np.log2(p_w1_w2 / (p_w1 * p_w2))\n",
        "    ppmi_matrix[(w1, w2)] = max(pmi, 0)\n",
        "\n",
        "print(\"PPMI Matrix:\", ppmi_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFVNDuz14_Xq",
        "outputId": "45c56453-61da-4dd6-cdff-3eccafa7b636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI Matrix: {('dog', 'cat'): 1.0, ('cat', 'bark'): 1.0, ('bark', 'meow'): 2.0, ('meow', 'dog'): 1.0, ('dog', 'bark'): 1.0, ('meow', 'cat'): 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER Visualization with Displacy for Legal Contracts"
      ],
      "metadata": {
        "id": "GeySqBBc7wKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "iNa2Skgu70he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(\"This Agreement is made on 4th April 2023 between Apple Inc. and John Doe.\")\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "twXzIHiI5Rn3",
        "outputId": "98eb8408-5235-4a89-81ec-181401d2b9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This Agreement is made on \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    4th\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    April 2023\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " between \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple Inc.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    John Doe\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Pattern Matching (e.g., Date + Entity)"
      ],
      "metadata": {
        "id": "6CVPmwEU74-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{\"ENT_TYPE\": \"DATE\"}, {\"LOWER\": \"between\"}, {\"ENT_TYPE\": \"ORG\"}]\n",
        "matcher.add(\"DATE_CONTRACT_PATTERN\", [pattern])\n",
        "\n",
        "doc = nlp(\"Signed on 5th March 2024 between Microsoft and John.\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    print(doc[start:end].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Ke3ztj3H73LF",
        "outputId": "16d9bf29-2666-4a65-c940-0d7569fc2973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nlp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a5eaf786dc13>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ENT_TYPE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"DATE\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"LOWER\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"between\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ENT_TYPE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ORG\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DATE_CONTRACT_PATTERN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Clustering using spaCy Vectors"
      ],
      "metadata": {
        "id": "_B9Q3E_p8QNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "sentences = [\"The cat sits on the mat\", \"Dogs bark loudly\", \"The dog is in the yard\"]\n",
        "docs = [nlp(sent) for sent in sentences]\n",
        "X = np.array([doc.vector for doc in docs])\n",
        "\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(X)\n",
        "\n",
        "for i, label in enumerate(kmeans.labels_):\n",
        "    print(f\"Cluster {label}: {sentences[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKaU70wZ8T0g",
        "outputId": "41edc5d7-a36e-4bcd-d835-632acac6f92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0: The cat sits on the mat\n",
            "Cluster 1: Dogs bark loudly\n",
            "Cluster 0: The dog is in the yard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Grained NER Training (Custom Labels)"
      ],
      "metadata": {
        "id": "AGHDjFtU8qFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Start from a blank English model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add NER pipeline component\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "ner.add_label(\"ORG\")\n",
        "ner.add_label(\"PRODUCT\")\n",
        "\n",
        "# Create a sample training set with diverse examples\n",
        "TRAIN_DATA = [\n",
        "    (\"Apple released the new Vision Pro.\", {\"entities\": [(0, 5, \"ORG\"), (24, 34, \"PRODUCT\")]}),\n",
        "    (\"Microsoft launched Surface Laptop.\", {\"entities\": [(0, 9, \"ORG\"), (19, 34, \"PRODUCT\")]}),\n",
        "    (\"Google unveiled the Pixel 8 phone.\", {\"entities\": [(0, 6, \"ORG\"), (19, 29, \"PRODUCT\")]}),\n",
        "    (\"Apple Vision Pro is a mixed reality headset.\", {\"entities\": [(0, 5, \"ORG\"), (6, 16, \"PRODUCT\")]}),\n",
        "    (\"Amazon presented the new Echo Show 10.\", {\"entities\": [(0, 6, \"ORG\"), (25, 39, \"PRODUCT\")]}),\n",
        "]\n",
        "\n",
        "# Convert to Example objects\n",
        "examples = []\n",
        "for text, annots in TRAIN_DATA:\n",
        "    doc = nlp.make_doc(text)\n",
        "    examples.append(Example.from_dict(doc, annots))\n",
        "\n",
        "# Training loop\n",
        "optimizer = nlp.begin_training()\n",
        "for i in range(30):  # More iterations for better training\n",
        "    nlp.update(examples, sgd=optimizer)\n",
        "\n",
        "# Test\n",
        "test_doc = nlp(\"Apple introduced the Vision Pro headset today.\")\n",
        "print([(ent.text, ent.label_) for ent in test_doc.ents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IedzbNXz8nNP",
        "outputId": "b2936bac-fc36-4478-c3f4-31b4bfc50a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'ORG'), ('Vision Pro', 'PRODUCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uju7fv_E8rwP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}