{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BybBSMyM4q3i",
        "outputId": "233b120e-d3b1-4b89-cbb5-c2aa8389d6dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize stemmers and lemmatizers\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample corpus from different domains\n",
        "corpus = {\n",
        "    \"news\": \"The economic downturn is affecting global markets. Investors are reconsidering their portfolios.\",\n",
        "    \"wikipedia\": \"The mitochondrion is often referred to as the powerhouse of the cell.\",\n",
        "    \"science_paper\": \"In recent studies, convolutional neural networks have demonstrated state-of-the-art performance in image classification tasks.\"\n",
        "}\n",
        "\n",
        "# Tokenize corpus\n",
        "tokenized_corpus = {key: nltk.word_tokenize(value) for key, value in corpus.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_stemming(text_tokens, stemmer):\n",
        "    return [stemmer.stem(word) for word in text_tokens]\n",
        "\n",
        "for domain, tokens in tokenized_corpus.items():\n",
        "    print(f\"\\n**{domain.upper()} DOMAIN**\")\n",
        "    print(\"Porter Stemming:\", apply_stemming(tokens, porter))\n",
        "    print(\"Snowball Stemming:\", apply_stemming(tokens, snowball))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlhJ_m-s4wlu",
        "outputId": "24ea078e-0c37-48c2-88d4-c0bf9e168339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**NEWS DOMAIN**\n",
            "Porter Stemming: ['the', 'econom', 'downturn', 'is', 'affect', 'global', 'market', '.', 'investor', 'are', 'reconsid', 'their', 'portfolio', '.']\n",
            "Snowball Stemming: ['the', 'econom', 'downturn', 'is', 'affect', 'global', 'market', '.', 'investor', 'are', 'reconsid', 'their', 'portfolio', '.']\n",
            "\n",
            "**WIKIPEDIA DOMAIN**\n",
            "Porter Stemming: ['the', 'mitochondrion', 'is', 'often', 'refer', 'to', 'as', 'the', 'powerhous', 'of', 'the', 'cell', '.']\n",
            "Snowball Stemming: ['the', 'mitochondrion', 'is', 'often', 'refer', 'to', 'as', 'the', 'powerhous', 'of', 'the', 'cell', '.']\n",
            "\n",
            "**SCIENCE_PAPER DOMAIN**\n",
            "Porter Stemming: ['in', 'recent', 'studi', ',', 'convolut', 'neural', 'network', 'have', 'demonstr', 'state-of-the-art', 'perform', 'in', 'imag', 'classif', 'task', '.']\n",
            "Snowball Stemming: ['in', 'recent', 'studi', ',', 'convolut', 'neural', 'network', 'have', 'demonstr', 'state-of-the-art', 'perform', 'in', 'imag', 'classif', 'task', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_lemmatization(text_tokens, lemmatizer, pos=wordnet.VERB):\n",
        "    return [lemmatizer.lemmatize(word, pos) for word in text_tokens]\n",
        "\n",
        "for domain, tokens in tokenized_corpus.items():\n",
        "    print(f\"\\n**{domain.upper()} DOMAIN**\")\n",
        "    print(\"NLTK Lemmatization:\", apply_lemmatization(tokens, lemmatizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2kQbpTB4yQ2",
        "outputId": "61a76d92-ac93-4e14-d356-1f162c0a6063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**NEWS DOMAIN**\n",
            "NLTK Lemmatization: ['The', 'economic', 'downturn', 'be', 'affect', 'global', 'market', '.', 'Investors', 'be', 'reconsider', 'their', 'portfolios', '.']\n",
            "\n",
            "**WIKIPEDIA DOMAIN**\n",
            "NLTK Lemmatization: ['The', 'mitochondrion', 'be', 'often', 'refer', 'to', 'as', 'the', 'powerhouse', 'of', 'the', 'cell', '.']\n",
            "\n",
            "**SCIENCE_PAPER DOMAIN**\n",
            "NLTK Lemmatization: ['In', 'recent', 'study', ',', 'convolutional', 'neural', 'network', 'have', 'demonstrate', 'state-of-the-art', 'performance', 'in', 'image', 'classification', 'task', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_lemmatization(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "for domain, text in corpus.items():\n",
        "    print(f\"\\n**{domain.upper()} DOMAIN**\")\n",
        "    print(\"SpaCy Lemmatization:\", spacy_lemmatization(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iesitCkv4zm6",
        "outputId": "1314ebdf-ca96-4a22-d5c2-7e7924eaee8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "**NEWS DOMAIN**\n",
            "SpaCy Lemmatization: ['the', 'economic', 'downturn', 'be', 'affect', 'global', 'market', '.', 'investor', 'be', 'reconsider', 'their', 'portfolio', '.']\n",
            "\n",
            "**WIKIPEDIA DOMAIN**\n",
            "SpaCy Lemmatization: ['the', 'mitochondrion', 'be', 'often', 'refer', 'to', 'as', 'the', 'powerhouse', 'of', 'the', 'cell', '.']\n",
            "\n",
            "**SCIENCE_PAPER DOMAIN**\n",
            "SpaCy Lemmatization: ['in', 'recent', 'study', ',', 'convolutional', 'neural', 'network', 'have', 'demonstrate', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance', 'in', 'image', 'classification', 'task', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "GEN_Z_DICT = {\n",
        "    \"money\": \"bread\", \"work\": \"grind\", \"friend\": \"bestie\", \"tired\": \"exhausted af\",\n",
        "    \"smart\": \"big-brain\", \"cool\": \"vibe check passed\", \"boring\": \"mid\",\n",
        "    \"excited\": \"hyped\", \"love\": \"obsessed\", \"understand\": \"get it, bestie\",\n",
        "    \"bad\": \"sus\", \"good\": \"fire\", \"funny\": \"lowkey hilarious\", \"amazing\": \"goated\",\n",
        "}\n",
        "\n",
        "def normal_mode(text):\n",
        "    \"\"\"Returns the text in its normal form after lemmatization.\"\"\"\n",
        "    return \" \".join([lemmatizer.lemmatize(w) for w in word_tokenize(text)])\n",
        "\n",
        "def gen_z_mode(text):\n",
        "    \"\"\"Converts text into Gen Z slang.\"\"\"\n",
        "    words = word_tokenize(text.lower())\n",
        "    return \" \".join([GEN_Z_DICT.get(w, w) for w in words])\n",
        "\n",
        "def translate():\n",
        "    text = input(\"\\nðŸ’¬ Enter a sentence: \")\n",
        "    print(f\"\\nâœ… **Normal:** {normal_mode(text)}\")\n",
        "    print(f\"\\nðŸ”¥ **Gen Z Mode:** {gen_z_mode(text)}\\n\")\n",
        "\n",
        "translate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coqBLgrlli9T",
        "outputId": "e585fd67-3931-4da9-9e0b-ddfbe62c34e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¬ Enter a sentence: I love my word, but it gets boring sometimes\n",
            "\n",
            "âœ… **Normal:** I love my word , but it get boring sometimes\n",
            "\n",
            "ðŸ”¥ **Gen Z Mode:** i obsessed my word , but it gets mid sometimes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "GEN_Z_DICT = {\n",
        "    \"money\": \"bread\", \"work\": \"grind\", \"friend\": \"bestie\", \"tired\": \"exhausted af\",\n",
        "    \"smart\": \"big-brain\", \"cool\": \"vibe check passed\", \"boring\": \"mid\",\n",
        "    \"excited\": \"hyped\", \"love\": \"obsessed\", \"understand\": \"get it, bestie\",\n",
        "    \"bad\": \"sus\", \"good\": \"fire\", \"funny\": \"lowkey hilarious\", \"amazing\": \"goated\",\n",
        "}\n",
        "\n",
        "BOOMER_DICT = {\n",
        "    \"money\": \"hard-earned cash\", \"work\": \"a 9-to-5 job\", \"friend\": \"pal\", \"tired\": \"worn out\",\n",
        "    \"smart\": \"book smart\", \"cool\": \"neat\", \"boring\": \"dull\",\n",
        "    \"excited\": \"thrilled\", \"love\": \"truly admire\", \"understand\": \"comprehend\",\n",
        "    \"bad\": \"not up to the mark\", \"good\": \"decent\", \"funny\": \"a real hoot\", \"amazing\": \"astonishing\",\n",
        "}\n",
        "\n",
        "CORPORATE_DICT = {\n",
        "    \"money\": \"capital\", \"work\": \"workflow optimization\", \"friend\": \"stakeholder\", \"tired\": \"resource depletion\",\n",
        "    \"smart\": \"data-driven\", \"cool\": \"innovative\", \"boring\": \"low engagement\",\n",
        "    \"excited\": \"strategically motivated\", \"love\": \"synergize with\", \"understand\": \"leverage insights\",\n",
        "    \"bad\": \"off-track\", \"good\": \"best-in-class\", \"funny\": \"engagement-boosting\", \"amazing\": \"disruptive\",\n",
        "}\n",
        "\n",
        "def translate_mode(text, dictionary):\n",
        "    \"\"\"Converts text using a given slang dictionary.\"\"\"\n",
        "    words = word_tokenize(text.lower())\n",
        "    return \" \".join([dictionary.get(w, w) for w in words])\n",
        "\n",
        "def translate():\n",
        "    text = input(\"\\nðŸ’¬ Enter a sentence: \")\n",
        "    print(f\"\\nâœ… **Normal:** {text}\")\n",
        "    print(f\"\\nðŸ”¥ **Gen Z Mode:** {translate_mode(text, GEN_Z_DICT)}\")\n",
        "    print(f\"\\nðŸ‘´ **Boomer Mode:** {translate_mode(text, BOOMER_DICT)}\")\n",
        "    print(f\"\\nðŸ’¼ **Corporate Mode:** {translate_mode(text, CORPORATE_DICT)}\\n\")\n",
        "\n",
        "translate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZs4M6TDoBdg",
        "outputId": "a113e0a5-be69-4f78-ec84-4c93101fea10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¬ Enter a sentence: I love my work but it can be boring sometimes\n",
            "\n",
            "âœ… **Normal:** I love my work but it can be boring sometimes\n",
            "\n",
            "ðŸ”¥ **Gen Z Mode:** i obsessed my grind but it can be mid sometimes\n",
            "\n",
            "ðŸ‘´ **Boomer Mode:** i truly admire my a 9-to-5 job but it can be dull sometimes\n",
            "\n",
            "ðŸ’¼ **Corporate Mode:** i synergize with my workflow optimization but it can be low engagement sometimes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7OOJHkNxoN9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}