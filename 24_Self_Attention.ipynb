{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled Dot Product Attention"
      ],
      "metadata": {
        "id": "ThDKMPtMzwAx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwnqi4yTzsgr",
        "outputId": "34da1990-9c72-4fec-982b-f6f508031a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of Attention: [[0.80222419 0.59888791]]\n",
            "Attention Weights: [[0.40111209 0.19777581 0.40111209]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Compute dot product of query and keys\n",
        "def compute_scores(Q, K):\n",
        "    return np.dot(Q, K.T)\n",
        "\n",
        "# Step 2: Scale the scores by the square root of the dimension of keys\n",
        "def scale_scores(scores, d_k):\n",
        "    return scores / np.sqrt(d_k)\n",
        "\n",
        "# Step 3: Apply softmax function to the scores\n",
        "def softmax(scores):\n",
        "    exp_scores = np.exp(scores - np.max(scores))  # Numerical stability trick\n",
        "    return exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
        "\n",
        "# Step 4: Compute the weighted sum of the values\n",
        "def compute_attention_output(attention_weights, V):\n",
        "    return np.dot(attention_weights, V)\n",
        "\n",
        "# Main attention function\n",
        "def attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]  # Assuming query and key have same dimension\n",
        "    scores = compute_scores(Q, K)\n",
        "    scaled_scores = scale_scores(scores, d_k)\n",
        "    attention_weights = softmax(scaled_scores)\n",
        "    output = compute_attention_output(attention_weights, V)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Example input: query, keys, and values\n",
        "Q = np.array([[1, 0]])  # Query\n",
        "K = np.array([[1, 0], [0, 1], [1, 1]])  # Keys\n",
        "V = np.array([[1, 1], [0, 1], [1, 0]])  # Values\n",
        "\n",
        "# Apply attention\n",
        "output, attention_weights = attention(Q, K, V)\n",
        "\n",
        "print(\"Output of Attention:\", output)\n",
        "print(\"Attention Weights:\", attention_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification"
      ],
      "metadata": {
        "id": "xx1NkZb9z73C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "sentences = [\n",
        "    'I love programming',\n",
        "    'I hate bugs',\n",
        "    'bugs are annoying',\n",
        "    'programming is awesome',\n",
        "    'I enjoy coding',\n",
        "    'I despise errors',\n",
        "    'errors make me sad',\n",
        "    'coding makes me happy'\n",
        "]\n",
        "labels = [1, 0, 0, 1, 1, 0, 0, 1]  # 1 = Positive, 0 = Negative\n",
        "\n",
        "# Build vocabulary\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # Add 1 for padding\n",
        "\n",
        "# Convert to sequences and pad\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "y = np.array(labels)\n",
        "\n",
        "# Parameters\n",
        "embedding_dim = 16\n",
        "hidden_dim = 16\n",
        "output_dim = 2\n",
        "maxlen = X.shape[1]\n",
        "\n",
        "# Self-Attention layer\n",
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.W_q = tf.keras.layers.Dense(hidden_dim)\n",
        "        self.W_k = tf.keras.layers.Dense(hidden_dim)\n",
        "        self.W_v = tf.keras.layers.Dense(hidden_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Q = self.W_q(inputs)\n",
        "        K = self.W_k(inputs)\n",
        "        V = self.W_v(inputs)\n",
        "        scores = tf.matmul(Q, K, transpose_b=True)\n",
        "        dim_k = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "        scaled_scores = scores / tf.math.sqrt(dim_k)\n",
        "        attention_weights = tf.nn.softmax(scaled_scores, axis=-1)\n",
        "        context = tf.matmul(attention_weights, V)\n",
        "        return context\n",
        "\n",
        "# Build model\n",
        "inputs = tf.keras.Input(shape=(maxlen,))\n",
        "x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "x = SelfAttention(hidden_dim)(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x)  # âœ… Fixed\n",
        "x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(output_dim, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train\n",
        "model.fit(X, y, epochs=30, verbose=1)\n",
        "\n",
        "# Prediction function\n",
        "def predict_sentiment(text):\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    padded = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=maxlen, padding='post')\n",
        "    probs = model.predict(padded)[0]\n",
        "    label = np.argmax(probs)\n",
        "    sentiment = \"Positive ğŸ˜Š\" if label == 1 else \"Negative ğŸ˜’\"\n",
        "    print(f\"Input: {text}\")\n",
        "    print(f\"Prediction: {sentiment} | Probabilities: {probs}\")\n",
        "\n",
        "# Test predictions\n",
        "predict_sentiment(\"I love coding\")\n",
        "predict_sentiment(\"bugs make me cry\")\n",
        "predict_sentiment(\"programming is fun\")\n",
        "predict_sentiment(\"I hate syntax errors\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQdEvnNN4Tnp",
        "outputId": "b5b84dad-1f16-47db-da72-85600df0d058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6250 - loss: 0.6921\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6898\n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8750 - loss: 0.6880\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8750 - loss: 0.6862\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8750 - loss: 0.6846\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8750 - loss: 0.6829\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.6812\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.6796\n",
            "Epoch 9/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.6779\n",
            "Epoch 10/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.6761\n",
            "Epoch 11/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.6740\n",
            "Epoch 12/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6718\n",
            "Epoch 13/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.6694\n",
            "Epoch 14/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.6668\n",
            "Epoch 15/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.6640\n",
            "Epoch 16/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.6610\n",
            "Epoch 17/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6578\n",
            "Epoch 18/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.6542\n",
            "Epoch 19/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.6502\n",
            "Epoch 20/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.6460\n",
            "Epoch 21/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.6414\n",
            "Epoch 22/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.6367\n",
            "Epoch 23/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.6318\n",
            "Epoch 24/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.6266\n",
            "Epoch 25/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.6211\n",
            "Epoch 26/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.6155\n",
            "Epoch 27/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.6096\n",
            "Epoch 28/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.6035\n",
            "Epoch 29/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.5972\n",
            "Epoch 30/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.5905\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
            "Input: I love coding\n",
            "Prediction: Positive ğŸ˜Š | Probabilities: [0.45006958 0.54993033]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Input: bugs make me cry\n",
            "Prediction: Negative ğŸ˜’ | Probabilities: [0.5440259  0.45597413]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Input: programming is fun\n",
            "Prediction: Positive ğŸ˜Š | Probabilities: [0.45133147 0.54866856]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Input: I hate syntax errors\n",
            "Prediction: Negative ğŸ˜’ | Probabilities: [0.5530318  0.44696823]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pSR-sS5o4zCD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}