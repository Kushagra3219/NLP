{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SFEvIoGfxZk3",
        "vGO52hcJxeRL",
        "j_B9lGqnxisz",
        "-rHgGKPS3USr",
        "8MmuWeMd3ZDV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Glove"
      ],
      "metadata": {
        "id": "SFEvIoGfxZk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gensim"
      ],
      "metadata": {
        "collapsed": true,
        "id": "s3Wl-RBaxmsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load word embeddings (can replace with your own Word2Vec/GloVe)\n",
        "model = api.load('glove-wiki-gigaword-100')  # or 'word2vec-google-news-300'\n",
        "\n",
        "def get_sentence_vector(sentence):\n",
        "    words = [word for word in sentence.lower().split() if word in model]\n",
        "    if not words:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean([model[word] for word in words], axis=0)"
      ],
      "metadata": {
        "id": "d0WO6eIstwAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmenting similar tasks"
      ],
      "metadata": {
        "id": "vGO52hcJxeRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = [\n",
        "    \"clean data\", \"build model\", \"train algorithm\", \"remove outliers\",\n",
        "    \"deploy model\", \"visualize data\", \"tune hyperparameters\", \"generate report\"\n",
        "]\n",
        "\n",
        "task_vectors = [get_sentence_vector(task) for task in tasks]\n",
        "\n",
        "# Use KMeans clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "labels = kmeans.fit_predict(task_vectors)\n",
        "\n",
        "# Print grouped tasks\n",
        "for i, label in enumerate(labels):\n",
        "    print(f\"Cluster {label}: {tasks[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd0ZD4KUtv-T",
        "outputId": "73a3f4a0-cfe4-4464-c8fa-d8369c3080bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 1: clean data\n",
            "Cluster 0: build model\n",
            "Cluster 0: train algorithm\n",
            "Cluster 1: remove outliers\n",
            "Cluster 0: deploy model\n",
            "Cluster 1: visualize data\n",
            "Cluster 2: tune hyperparameters\n",
            "Cluster 1: generate report\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking accuracy in summary"
      ],
      "metadata": {
        "id": "j_B9lGqnxisz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_summary_accuracy(original_text, summary_text):\n",
        "    orig_words = [word for word in original_text.lower().split() if word in model]\n",
        "    summary_words = [word for word in summary_text.lower().split() if word in model]\n",
        "\n",
        "    orig_vecs = np.array([model[word] for word in orig_words])\n",
        "    summary_vecs = np.array([model[word] for word in summary_words])\n",
        "\n",
        "    similarities = cosine_similarity(summary_vecs, orig_vecs)\n",
        "    coverage = np.mean(np.max(similarities, axis=1))  # max sim for each summary word\n",
        "    return coverage\n",
        "\n",
        "original = \"The quick brown fox jumps over the lazy dog and runs into the forest.\"\n",
        "summary = \"A brown fox jumps over a lazy dog.\"\n",
        "\n",
        "accuracy_score = check_summary_accuracy(original, summary)\n",
        "print(\"Summary accuracy score (cosine-based):\", accuracy_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rezQj2ffxgVi",
        "outputId": "b9dd685e-a0dd-43c5-863f-a9be749f3d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary accuracy score (cosine-based): 0.93599373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Clustering"
      ],
      "metadata": {
        "id": "-rHgGKPS3USr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Dogs are wonderful pets\",\n",
        "    \"Cats are independent animals\",\n",
        "    \"Dogs love to play fetch\",\n",
        "    \"Cats love to nap\",\n",
        "    \"Football is a great sport\",\n",
        "    \"Soccer is popular worldwide\",\n",
        "]\n",
        "\n",
        "doc_vectors = [get_sentence_vector(doc) for doc in documents]\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "clusters = kmeans.fit_predict(doc_vectors)\n",
        "\n",
        "for i, label in enumerate(clusters):\n",
        "    print(f\"Cluster {label}: {documents[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpvuxJA3xlGV",
        "outputId": "150fc498-fe76-48b4-9ac0-5f67adbd152e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0: Dogs are wonderful pets\n",
            "Cluster 0: Cats are independent animals\n",
            "Cluster 0: Dogs love to play fetch\n",
            "Cluster 0: Cats love to nap\n",
            "Cluster 1: Football is a great sport\n",
            "Cluster 1: Soccer is popular worldwide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Search"
      ],
      "metadata": {
        "id": "8MmuWeMd3ZDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"How to train a neural network\",\n",
        "    \"Ways to clean data for machine learning\",\n",
        "    \"Data visualization techniques\",\n",
        "    \"Best practices for model deployment\",\n",
        "]\n",
        "\n",
        "query = \"visualizing data\"\n",
        "query_vec = get_sentence_vector(query)\n",
        "\n",
        "corpus_vecs = [get_sentence_vector(doc) for doc in corpus]\n",
        "sims = cosine_similarity([query_vec], corpus_vecs)[0]\n",
        "\n",
        "results = sorted(zip(corpus, sims), key=lambda x: x[1], reverse=True)\n",
        "print(\"Semantic Search Results:\")\n",
        "for doc, score in results:\n",
        "    print(f\"{score:.3f}: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oaz7CqLB3V3v",
        "outputId": "90655525-db41-4d66-a3bb-e8d9ffa6daaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic Search Results:\n",
            "0.829: Data visualization techniques\n",
            "0.634: Ways to clean data for machine learning\n",
            "0.515: How to train a neural network\n",
            "0.438: Best practices for model deployment\n"
          ]
        }
      ]
    }
  ]
}